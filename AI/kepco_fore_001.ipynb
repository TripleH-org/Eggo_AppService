{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/my/txwbd_k97ndchljktlwzr3k00000gp/T/ipykernel_68951/1463553636.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dropout, Dense\n",
    "import keras.backend as K \n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "\n",
    "def readData( sTmStt, sTmEnd ):\n",
    "    cursor = data_db.cursor()\n",
    "    sql = \"SELECT mrdTm, validVol FROM naju_sum WHERE mrdTm >= '%s' AND mrdTm <= '%s'\" % ( sTmStt, sTmEnd )\n",
    "    cursor.execute( sql )\n",
    "    res = cursor.fetchall()\n",
    "    df = pd.DataFrame(res)\n",
    "    df.columns = [\"ds\", \"y\"]\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"], format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    #df[\"ds\"] += pd.to_timedelta(9, unit='h')\n",
    "    return df\n",
    "\n",
    "def readWeather(sTmStt, sTmEnd):\n",
    "    # humi 0 ~ 6, rain 0 ~ 65, temp -14.1 ~ 35.9, wind 0 ~ 7.1\n",
    "    cursor = data_db.cursor()\n",
    "    sql = \"SELECT ymdt, temp, rain, wind, humi FROM naju_wthr WHERE ymdt >= '%s' AND ymdt <= '%s'\" % (sTmStt, sTmEnd) \n",
    "    cursor.execute( sql )\n",
    "    res = cursor.fetchall()\n",
    "    df = pd.DataFrame(res)\n",
    "    df.columns = [\"ds\", \"temp\", \"rain\", \"wind\", \"humi\"] # \n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"], format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    #df[\"ds\"] += pd.to_timedelta(9, unit='h')\n",
    "    # df[\"humi\"] = df[\"humi\"] * 0.1\n",
    "    df[\"day\"] = df[\"ds\"].apply(lambda v: setDayNight(v))\n",
    "    df[\"week\"] = df[\"ds\"].apply(lambda v: setHolidays(v))\n",
    "    return df\n",
    "\n",
    "def setDayNight(v):\n",
    "    if v.hour >= 7 and v.hour <= 18 :\n",
    "        ret = 10\n",
    "    else:\n",
    "        ret = 0\n",
    "    return ret\n",
    "\n",
    "#def setDayNight( df ):\n",
    "#    for i in df.index:\n",
    "#        if df.loc[i, 'ds'].hour >= 7 and df.loc[i, 'ds'].hour <= 18 :\n",
    "#            df.loc[i, 'day'] = 10\n",
    "#        else:\n",
    "#            df.loc[i, 'day'] = 0\n",
    "\n",
    "def setHolidays( v ):\n",
    "    if v.weekday() >= 5 :\n",
    "        ret = 10\n",
    "    else:\n",
    "        ret = 0\n",
    "    return ret\n",
    "\n",
    "def mape_not_zero(y_true, y_pred):\n",
    "    mape = 0\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        if t != 0:\n",
    "            mape += np.abs((t - p) / t)\n",
    "        elif p != 0:\n",
    "            mape += np.abs((t - p) / p)\n",
    "\n",
    "    mape /= len(y_true)\n",
    "    return np.mean(mape) * 100\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    mse = (np.square(y_true - y_pred)).mean(axis=0)\n",
    "    return np.sqrt(mse) \n",
    "\n",
    "#def setHolidays( df ):\n",
    "#   for i in df.index:\n",
    "#        if df.loc[i, 'ds'].weekday() >= 5 :\n",
    "#            df.loc[i, 'week'] = 10\n",
    "#        else:\n",
    "#            df.loc[i, 'week'] = 0\n",
    "\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_db = pymysql.connect(\n",
    "    user='itman', \n",
    "    passwd='itman0808!', \n",
    "    host='192.168.0.126', \n",
    "    db='newkepco', \n",
    "    charset='utf8'\n",
    ")\n",
    "\n",
    "work_db = pymysql.connect(\n",
    "    user='itman', \n",
    "    passwd='itman1234', \n",
    "    host='192.168.0.5', \n",
    "    db='car_chrg2', \n",
    "    charset='utf8'\n",
    ")\n",
    "\n",
    "work_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allX = readWeather(\"2017-01-04 00:00:00\", \"2018-12-27 23:59:59\")\n",
    "allY = readData( \"2017-01-04 00:00:00\", \"2018-12-27 23:59:59\")\n",
    "print( allX.count(), allY.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in allX.index:\n",
    "    if i >= 24:\n",
    "        allX.loc[i, 'avg'] = allY.loc[i-24, 'y']\n",
    "    else:\n",
    "        allX.loc[i, 'avg'] = allY.loc[i, 'y']\n",
    "    if i >= 48:\n",
    "        allX.loc[i, 'avg2'] = allY.loc[i-48, 'y']\n",
    "    else:\n",
    "        allX.loc[i, 'avg2'] = allY.loc[i, 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allX.set_index('ds', inplace=True)\n",
    "allX.sort_values('ds', inplace=True)\n",
    "allY.set_index('ds', inplace=True)\n",
    "allY.sort_values('ds', inplace=True)\n",
    "\n",
    "trainX = allX[:\"2018-11-30 21:00:00\"].copy()\n",
    "testX = allX[\"2018-11-30 22:00:00\":].copy()\n",
    "trainY = allY[:\"2018-11-30 21:00:00\"].copy()\n",
    "testY = allY[\"2018-11-30 22:00:00\":].copy()\n",
    "\n",
    "testX.head()\n",
    "# print( type(trainX.loc[\"2017-01-01 00:00:00\",\"temp\"]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsize = len(testY)\n",
    "timesteps = seq_length = 2 # 2시간을 학습으로.\n",
    "batch_size = 1\n",
    "data_dim = 8\n",
    "hidden_dim = 8\n",
    "output_dim = 1\n",
    "learing_rate = 0.0005\n",
    "iterations = 50000\n",
    "\n",
    "scaler1 = RobustScaler()\n",
    "scaler2 = RobustScaler()\n",
    "scaler3 = RobustScaler()\n",
    "scaler4 = RobustScaler()\n",
    "trax = scaler1.fit_transform(trainX.values)\n",
    "tray = scaler2.fit_transform(trainY.values)\n",
    "tstx = scaler3.fit_transform(testX.values)\n",
    "tsty = scaler4.fit_transform(testY.values)\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(tray) - seq_length):\n",
    "    _x = np.copy(trax[i:i + seq_length + 1])\n",
    "    _y = [tray[i + seq_length]]\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "traX = np.array(dataX[:])\n",
    "traY = np.array(dataY[:])[:,:,0]    \n",
    "    \n",
    "data2X = []\n",
    "data2Y = []\n",
    "for i in range(0, len(tsty) - seq_length):\n",
    "    _x = np.copy(tstx[i:i + seq_length + 1])\n",
    "    _y = [tsty[i + seq_length]]\n",
    "    data2X.append(_x)\n",
    "    data2Y.append(_y)\n",
    "    \n",
    "tstX = np.array(data2X[:])\n",
    "tstY = np.array(data2Y[:])[:,:,0]    \n",
    "print( traX.shape, traY.shape, tstX.shape, tstY.shape, len(tsty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()  \n",
    "model.add(GRU(64, batch_input_shape=(batch_size, seq_length+1, data_dim),\n",
    "              stateful=True, return_sequences=True,\n",
    "              dropout=.1, recurrent_dropout=.2))  \n",
    "model.add(GRU(32, stateful=True,\n",
    "              dropout=.2, recurrent_dropout=.5))  \n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam') \n",
    "early_stop = EarlyStopping(monitor='loss', patience=2, verbose=1)\n",
    "\n",
    "hist = model.fit(traX, traY, epochs=500,\n",
    "                 batch_size=batch_size, verbose=1, \n",
    "                 callbacks=[early_stop],\n",
    "                 validation_data=(tstX, tstY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmse = model.evaluate(tstX, tstY, batch_size=1)\n",
    "testmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(tstX, batch_size=1)\n",
    "\n",
    "preds = scaler4.inverse_transform(predictions)\n",
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = testY[-preds.size:].copy()\n",
    "result[\"yhat\"] = preds[:]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmape = mape_not_zero(result.y.values[:], result.yhat.values[:])\n",
    "testrmse = rmse(result.y.values[:], result.yhat.values[:])\n",
    "print(testmape, testrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.register_matplotlib_converters()\n",
    "result[0:80].plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = work_db.cursor()\n",
    "for i in result.index:\n",
    "    sql = \" INSERT INTO CC_USE_VOL_FORE ( FORE_TM, FORE_VAL, REAL_VAL ) VALUES ( '%s', '%s', '%s' ) \" \\\n",
    "        \" ON DUPLICATE KEY UPDATE FORE_VAL=VALUES(FORE_VAL), REAL_VAL=VALUES(REAL_VAL) \" \\\n",
    "        % ( i, result.loc[i,'yhat'], result.loc[i,'y'] ) \n",
    "    cursor.execute( sql )\n",
    "cursor.execute( \"COMMIT\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path = \"model/model_rnn_kepco_001.pkl\"\n",
    "with open(pkl_path, \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
